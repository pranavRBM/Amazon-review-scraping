{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers = {\n",
    "    'authority': 'www.amazon.in',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the \"user-agent\" to your user agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_query=\"nike+shoes+men\"\n",
    "base_url=\"https://www.amazon.in/s?k=\"\n",
    "url=base_url+search_query\n",
    "url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the ' search_query ' to do the scraping of different search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_response=requests.get(url,headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get '''\n",
    "search_response.status_code \n",
    "''' 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie={} # insert request cookies within{}\n",
    "def getAmazonSearch(search_query):\n",
    "    url=\"https://www.amazon.in/s?k=\"+search_query\n",
    "    print(url)\n",
    "    page=requests.get(url,headers=headers)\n",
    "    if page.status_code==200:\n",
    "        return page\n",
    "    else:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Searchasin(asin):\n",
    "    url=\"https://www.amazon.in/dp/\"+asin\n",
    "    print(url)\n",
    "    page=requests.get(url,cookies=cookie,headers=headers)\n",
    "    if page.status_code==200:\n",
    "        return page\n",
    "    else:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Searchreviews(review_link):\n",
    "    url=\"https://www.amazon.in\"+review_link\n",
    "    print(url)\n",
    "    page=requests.get(url,cookies=cookie,headers=headers)\n",
    "    if page.status_code==200:\n",
    "        return page\n",
    "    else:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "product_names=[]\n",
    "response=getAmazonSearch('nike+shoes+men')\n",
    "soup=BeautifulSoup(response.content)\n",
    "for i in soup.findAll(\"span\",{'class':'a-size-base-plus a-color-base a-text-normal'}): # the tag which is common for all the names of products\n",
    "    product_names.append(i.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_asin=[]\n",
    "response=getAmazonSearch('nike+shoes+men')\n",
    "soup=BeautifulSoup(response.content)\n",
    "for i in soup.findAll(\"div\",{'class':\"sg-col-4-of-24 sg-col-4-of-12 s-result-item s-asin sg-col-4-of-16 sg-col s-widget-spacing-small sg-col-4-of-20\"}):\n",
    "    data_asin.append(i['data-asin'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link=[]\n",
    "for i in range(len(data_asin)):\n",
    "    response=Searchasin(data_asin[i])\n",
    "    soup=BeautifulSoup(response.content)\n",
    "    for i in soup.findAll(\"a\",{'data-hook':\"see-all-reviews-link-foot\"}):\n",
    "        link.append(i['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for j in range(len(link)):\n",
    "    for k in range(10):\n",
    "        response = Searchreviews(link[j] + '&pageNumber=' + str(k))\n",
    "        soup = BeautifulSoup(response.content)\n",
    "        for i in soup.findAll(\"div\", {'data-hook': \"review\"}):\n",
    "            try:\n",
    "                name = i.select_one('[class=\"a-profile-name\"]').text.strip()\n",
    "            except Exception as e:\n",
    "                name = 'N/A'\n",
    "\n",
    "            try:\n",
    "                stars = i.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n",
    "            except Exception as e:\n",
    "                stars = 'N/A'   \n",
    "\n",
    "            try:\n",
    "                title = i.select_one('[data-hook=\"review-title\"]').text.strip().split('\\n')[1]\n",
    "            except Exception as e:\n",
    "                title = 'N/A'\n",
    "\n",
    "            try:\n",
    "                # Convert date str to dd/mm/yyy format\n",
    "                date = i.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n",
    "\n",
    "            except Exception as e:\n",
    "                date = 'N/A'\n",
    "\n",
    "            try:\n",
    "                description = i.select_one('[data-hook=\"review-body\"]').text.strip()\n",
    "            except Exception as e:\n",
    "                description = 'N/A'\n",
    "                \n",
    "            try:\n",
    "                verification = i.select_one('[data-hook=\"avp-badge\"]').text.strip()\n",
    "            except Exception as e:\n",
    "                verification = 'N/A'\n",
    "                \n",
    "            # create Dictionary with all review data \n",
    "            data_dict = {\n",
    "                'product':product_names[j],\n",
    "                'Name': name,\n",
    "                'Stars': stars,\n",
    "                'Title': title,\n",
    "                'Date': date,\n",
    "                'Verification': verification,\n",
    "                'Description': description\n",
    "            }\n",
    "\n",
    "            # Add Dictionary in master empty List\n",
    "            reviews.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data=pd.DataFrame.from_dict(reviews)\n",
    "pd.set_option('max_colwidth',800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.to_csv('Scraping reviews.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
